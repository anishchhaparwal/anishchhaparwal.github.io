---
layout: post
title: data pipeline with Prefect
---

## What are data pipelines?

My first day at my first job involved overlooking 1200 hatchback cars in a shed. I had been told that all of them were made in the last 3 days and were ready for dispatch. It was at this instance the true marvel of what Henry Ford had achieved in 1913 with the first moving assembly line had sunk in. Not only did he cut down the production time for each vehicle from 12 hours to about 93 minutes but also streamlined the entire process. The role of assembly lines is a perfect analogy for Data pipelines.

Data being the modern day oil makes Data flow one of the most critical operations in todays data-driven world. Unfortunately, dealing with data flow using manual steps is not only a laborious process but also a precarious one.

Unfortunately, data flow is precarious for a number of reasons: data can be corrupt, data sources may conflict, data maybe inconsistant across sources and one might not always have updated data. Even if we had the data ready we would have to transform it into our required format. Lastly, any person to have ever written code will know its great when script run fine but more often then not it encounters some issues. 

Data pipelines aims at addressing all the pitfalls mentioned above. It is software than eliminates manual steps and enables automated flow of data from one station to the next. It address the what where how and when for each station. It automates the processes of extracting, transforming, combining, validating, and loading data. Additionally it can process multiple streams at once along with [state handling](https://docs.prefect.io/core/concepts/notifications.html)

## Prefect: A modern data workflow engine

Prefect is a great data automation tool that can help in designing, building, testing and running powerful data applications. Apache airflow has been a front runner in workflow orchestration tooling space but the inception of prefect has been to address the short comings of airflow. For anyone familar with airflow [this article](https://medium.com/the-prefect-blog/why-not-airflow-4cfa423299c4) can providing a compelling reason to give prefect a shot.


## Exploring Prefect features with a working example:

#### Overview:
This project is meant to showcase how to develop a basic "ETL" using Prefect.io- a modern dataflow automation tool. For the problem statement we use public open dataset of chest X-ray and CT images of patients which are positive or suspected of COVID-19 or other viral and bacterial pneumonias (MERS, SARS, and ARDS) available [here](https://github.com/ieee8023/covid-chestxray-dataset)

You can find the code mentioned below at [git_repo](https://github.com/anishchhaparwal/ETL_with_prefect_pipelines)
Note: all utils fuctions used in ETL.py can be found under ETL_utils.py

#### Install prefect:
Prefect consists of several components where many can be used optionally. The central component is Prefect Core which is the basic framework to define flows and tasks including the possibility to run them locally. It is available in PyPI as a pip package and can be installed like the following into a virtual environment or globally.

```
pip install prefect
```

#### Extraction:
The extraction module involved purging the folder and cloning the git repo with our dataset. A conventional python script would look something like:
```
import git
from ETL_utils import filepath, purge_folder

def clone(dirpath,git_url):
    purge_folder(dirpath)
    git.Repo.clone_from(git_url, dirpath, progress=Progress())

if __name__ == "__main__":
    git_url = "https://github.com/ieee8023/covid-chestxray-dataset"
    dirpath = filepath("", "data")
    clone(dirpath,git_url)
```
Prefect being natively build in python can be used to convert the above code into a pipeline using just a few lines. 
First we add a @task decorator to the clone function. While using the decorator we can pass arguments like cache_for and max_retries which help avoid downloading the data each time incase of multiple runs and  retry the function incase it fails. The log_stdout support forwarding the output of stdout to a logger which gets created by default while running a flow.

The second step is to add a scheduler to trigger running the pipeline. It is optional and only added here to demonstrate cacheing the clone function. Scheduler can be configured in various ways, to know more you can read [here](https://docs.prefect.io/api/latest/schedules/schedules.html#schedule)

The third and last step would involve adding a flow, passing each user input into a parameter wrapper and running the flow using flow.run().

Thats it! you have now converted your script into a data pipeline.
```
# step 1
@task(cache_for=datetime.timedelta(minutes=8), log_stdout=True, max_retries=2, retry_delay=datetime.timedelta(seconds=30))
def clone(dirpath, git_url):
    """Purges dirpath and clones git_url into dirpath

    Args:
        dirpath ([type]): path where git repo is to be cloned
        git_url ([type]): url of git repo
    """
    purge_folder(dirpath)
    Repo.clone_from(git_url, dirpath, progress=Progress())


# step 2
schedule = IntervalSchedule(start_date=datetime.datetime.utcnow(), interval=datetime.timedelta(minutes=5))

# step 3
if __name__ == "__main__":
    with Flow("ETL", schedule) as flow:

        # Input Parameters
        git_url = Parameter("git_url", default="https://github.com/ieee8023/covid-chestxray-dataset")
        input_data = filepath("", "data")

        # Extraction
        clone(input_data, git_url)

    flow.run()
```
During the first run your log will look as follows:
Starting:
![_config.yml]({{ site.baseurl }}/images/prefect_1.PNG)
ending:
![_config.yml]({{ site.baseurl }}/images/prefect_2.PNG)

During the second run the pipeline will use the cached output instead of running the flow again. you log will look as follows:
![_config.yml]({{ site.baseurl }}/images/prefect_3.PNG)

Note: The cache is done in memory here. External locations can be configured for the same. for details [click here](https://docs.prefect.io/core/concepts/persistence.html#output-caching-based-on-a-file-target)


#### Transformation
Data transformation involves filtering/transforming data into required form. For the sake of simplicity, we will check metadata.csv for valid filepaths and filter images above size 1023.
```
@task
def filter_data(metadata_path, img_folder, results_folder):
    """Filters data for which images are present and above size 1023.

    Args:
        metadata_path ([str]): path where metadata.csv is present.
        img_folder ([str]): path where x-ray images are present.
        results_folder ([str]): path where final results are to be saved.

    Returns:
        [list]: list of image_paths
    """
    df = pd.read_csv(metadata_path)

    imgs_path = [file for file in os.listdir(img_folder)]
    df = df[df["filename"].isin(imgs_path)]

    for item in df["filename"]:
        df.loc[df["filename"] == item, "height"], df.loc[df["filename"] == item, "width"] = Image.open(
            os.path.join(img_folder, item)
        ).size

    df_included = df[(df.height > 1023) & (df.width > 1023)]
    df_included["fullfilepath"] = str(img_folder) + "/" + df["filename"].astype(str)

    purge_folder(results_folder)
    df_included.to_csv(os.path.join(results_folder, "included_csv_metadata.csv"))

    return df_included["fullfilepath"].tolist()


# step 3
if __name__ == "__main__":
    with Flow("ETL", schedule) as flow:

        # Input Parameters
        git_url = Parameter("git_url", default="https://github.com/ieee8023/covid-chestxray-dataset")
        input_data = filepath("", "data")
        metadata_path = Parameter("metadeta_path_file", default=filepath("data", "metadata.csv"))
        img_folder = Parameter("images_folder", default=filepath("data", "images"))
        results_folder = Parameter("result_folder", default=filepath("data", "results"))

        # Extraction
        clone_state = clone(input_data, git_url)

        # Transformation
        image_filepath = filter_data(metadata_path, img_folder, results_folder, upstream_tasks=[clone_state])

    flow.run()
``` 
In the task above we have linked the extraction and transformation functions by adding upstream_tasks as a argument to filter_data function in the flow. You can notice that there is no data link between clone and filter_data function but we are able to establish a connection using prefects [imperative API](https://docs.prefect.io/core/concepts/flows.html#imperative-api). This is particularly useful when combined with prefect triggers via which we can decide which functions to run based on the [state](https://docs.prefect.io/core/concepts/states.html#overview) of previous functions. To view example [click here](https://docs.prefect.io/api/latest/triggers.html#functions)

#### Loading:
In the loading module we are going to resize the images to 224x224. Here we are going to use prefect map feature instead of a loop. The advantages of map being each element of the list can be treated indivisually and does not need the whole list to be [processed](https://docs.prefect.io/core/concepts/mapping.html#iterated-mapping) for it being passed down to further task. Additionally elements of a mapped task can be processed in parallel by using libraries like [dask](https://examples.dask.org/applications/prefect-etl.html)

```
@task
def resize(image_path, results_folder):
    """resizing images to 224x224 and saving at desired location.

    Args:
        image_path ([str]): filepath of image to be ressized.
        results_folder ([str]): [description]
    """
    img = Image.open(image_path)
    img = img.resize((224, 224), Image.ANTIALIAS)
    img.save(os.path.join(results_folder, image_path))

if __name__ == "__main__":
    with Flow("ETL", schedule) as flow:
    
        # Input Parameters
        git_url = Parameter("git_url", default="https://github.com/ieee8023/covid-chestxray-dataset")
        input_data = filepath("", "data")
        metadata_path = Parameter("metadeta_path_file", default=filepath("data", "metadata.csv"))
        img_folder = Parameter("images_folder", default=filepath("data", "images"))
        results_folder = Parameter("result_folder", default=filepath("data", "results"))

        # Extraction
        clone_state = clone(input_data, git_url)

        # Transformation
        image_filepath = filter_data(metadata_path, img_folder, results_folder, upstream_tasks=[clone_state])

        # Load
        resize.map(file_metadata=image_filepath, results_folder=unmapped(results_folder))

    flow.run()
```

#### Deployment:
Now that we are done with the python script, its time to set up prefects neat UI.
Please ensure you have docker and docker-compose installed on your system.
You can setup the prefect UI using prefects CLI. start a terminal and enter the following:
```
prefect backend server
prefect server start
```
By default you can view the UI at http://localhost:8080. The UI will be communicating with a default apollo endpoint http://localhost:4200/graphql. If users should access Apollo at a different location (e.g. if you're running Prefect Server behind a proxy), you'll need to configure the UI to look at a different URL.
You can make the following changes to the config file found at ~/.prefect/config.toml
```
[server]
  [server.ui]
  apollo_url="http://localhost:4200/graphql"
```
Or you can directly set this in the UI home page:
![_config.yml]({{ site.baseurl }}/images/server-endpoint.png)

For more details on UI deployment [click here](https://docs.prefect.io/orchestration/server/deploy-local.html#ui-configuration)

Once the UI is running we need to start a project. you can do so by click on new project on the top right corner and naming it "ETL_Project"

Next we need an agent to communicate between the flows and UI. To start an agent start a new terminal and enter the following:
```
prefect backend server
prefect agent start -l myETL -p <path to source code>
```
Note: activate the virtual env which you want to run the flow to run in. The -l tag represents the unique label naming linking flows to agent while registering a flow. the -p import paths (modules/files) that are required for the flow to run.

Once the agent is setup you can view it on the UI as shown below:
"image"

The last step is to replace the flow.run() with the following:
```
from prefect.environments import LocalEnvironment

flow.environment = LocalEnvironment(labels="myETL")
flow.register(project_name="ETL_Project")
```

You can now find a flow named ETL under ETL_Projects. You can run the flow directly from UI using the run options. Additionally it also give you the options to alter the default parameters before the run.

"image"

The logs for each task can be directly accessed from the UI under logs tab as shown here:
"image"

#### Conclusion:
We have just scrathed the surface with the possibilities that prefect presents. Even though prefect can be overwheling at first its very intutive and can streamline large projects very quickly. Additionally, prefect provides great support for production-env with help of docker, kubernetes and storage buckets etc. It has a very active slack community where you shall always get a quick response. For anyone looking for a workflow orchestration system, I would highly recommend giving Prefect a try. 